# Tasks: Headless LLM Migration

**Input**: Design documents from `/specs/005-headless-llms/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: No tests explicitly requested in the feature specification.

**Organization**: Tasks are grouped by implementation phase to enable systematic migration from external LMStudio dependency to local Python embedding server.

## Format: `[ID] [P?] [Story?] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Rust application**: `localmind-rs/`
- **Python server**: `embedding-server/` (`embedding_server.py`, `pyproject.toml`, `.python-version`)
- **Startup scripts**: Repository root (`start_localmind.bat`, `start_localmind.sh`)

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and Python environment setup

- [x] T001 Verify `.python-version` file exists with content "3.13" (already present)
- [x] T002 [P] Create `pyproject.toml` in repository root with FastAPI, uvicorn, llama-cpp-python, huggingface-hub dependencies and mypy, ruff, types-* stubs in dev dependencies
- [x] T003 [P] Add reqwest and serde_json dependencies to `localmind-rs/Cargo.toml` (already present)

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

- [x] T004 Remove `localmind-rs/src/lmstudio.rs` file entirely
- [x] T005 [P] Remove lmstudio module export from `localmind-rs/src/lib.rs` (also removed ollama.rs)
- [x] T006 [P] Remove all LMStudio imports and references from `localmind-rs/src/main.rs`
- [x] T007 [P] Remove all completion/generation Tauri commands from `localmind-rs/src/main.rs` (removed generate_response, generate_response_stream, cancel_generation, chat_with_rag, get_ollama_models)
- [x] T008 [P] Remove all completion/generation endpoints from `localmind-rs/src/http_server.rs` (verified clean - no completion endpoints)
- [x] T009 Remove all completion/generation UI components from `ui/` directory (deleted AIPanel.svelte, removed synthesis button from SearchResults.svelte)

**Checkpoint**: Foundation ready - user story implementation can now begin

---

## Phase 3: User Story 3 - Local Embedding Generation (Priority: P1) üéØ TECHNICAL FOUNDATION

**Goal**: Implement Python FastAPI embedding server and Rust HTTP client to replace external LMStudio dependency

**Independent Test**: Start Python server manually, call `/health` and `/embed` endpoints with curl, verify 768-dim embeddings returned

### Python Server Implementation

- [x] T010 [P] [US3] Create `embedding-server/` directory and move `.python-version` and `pyproject.toml` into it
- [x] T011 [P] [US3] Create `embedding-server/embedding_server.py` with FastAPI app initialization
- [x] T012 [US3] Add EmbeddingRequest TypedDict to `embedding-server/embedding_server.py` (text: str)
- [x] T013 [US3] Add EmbeddingResponse TypedDict to `embedding-server/embedding_server.py` (embedding: list[float], model: str, dimension: int)
- [x] T014 [US3] Add HealthResponse TypedDict to `embedding-server/embedding_server.py` (status: str, model_loaded: bool)
- [x] T015 [US3] Add ErrorResponse TypedDict to `embedding-server/embedding_server.py` (error: str, detail: str | None)
- [x] T016 [US3] Implement model loading function in `embedding-server/embedding_server.py` using sentence-transformers for google/embeddinggemma-300M
- [x] T017 [US3] Implement GET `/health` endpoint in `embedding-server/embedding_server.py` returning HealthResponse
- [x] T018 [US3] Implement POST `/embed` endpoint in `embedding-server/embedding_server.py` accepting EmbeddingRequest and returning EmbeddingResponse
- [x] T019 [US3] Add request validation in `/embed` endpoint (empty text check, length limit <2000 chars)
- [x] T020 [US3] Add "loading" state handling in `/embed` endpoint (return 503 with Retry-After header)
- [x] T021 [US3] Add error handling and logging to `embedding-server/embedding_server.py` (startup, model loading, request processing)
- [x] T022 [US3] Add embedding dimension validation in `embedding-server/embedding_server.py` (assert len(embedding) == 768)
- [x] T023 [US3] Add OOM (out of memory) error handling in `embedding-server/embedding_server.py` with clear error message and graceful shutdown
- [x] T024 [US3] Add model cache validation in `embedding-server/embedding_server.py` (detect corrupted files, re-download on integrity check failure - handled by sentence-transformers automatically)
- [x] T025 [US3] Verify Python code passes `mypy --strict embedding-server/embedding_server.py`
- [x] T026 [US3] Verify Python code passes `ruff check embedding-server/embedding_server.py` and `ruff format --check embedding-server/embedding_server.py`

### Rust HTTP Client Implementation

- [x] T027 [P] [US3] Create `localmind-rs/src/local_embedding.rs` with module skeleton and imports
- [x] T028 [US3] Add EmbeddingRequest struct to `localmind-rs/src/local_embedding.rs` with serde derive
- [x] T029 [US3] Add EmbeddingResponse struct to `localmind-rs/src/local_embedding.rs` with serde derive
- [x] T030 [US3] Add ErrorResponse struct to `localmind-rs/src/local_embedding.rs` with serde derive
- [x] T031 [US3] Implement LocalEmbeddingClient struct in `localmind-rs/src/local_embedding.rs` with reqwest client
- [x] T032 [US3] Implement `new()` method for LocalEmbeddingClient in `localmind-rs/src/local_embedding.rs`
- [x] T033 [US3] Implement `generate_embedding()` async method in `localmind-rs/src/local_embedding.rs` (POST to localhost:$EMBEDDING_SERVER_PORT/embed, default 8000)
- [x] T034 [US3] Add retry logic with exponential backoff for "loading" responses (503 status) in `localmind-rs/src/local_embedding.rs`
- [x] T035 [US3] Add error handling for connection failures in `localmind-rs/src/local_embedding.rs`
- [x] T036 [US3] Add dimension validation (assert dimension == 768) in `localmind-rs/src/local_embedding.rs`
- [x] T037 [US3] Add doc comments to public functions in `localmind-rs/src/local_embedding.rs`
- [x] T038 [US3] Export LocalEmbeddingClient from `localmind-rs/src/lib.rs`

**Checkpoint**: Python server and Rust client are both functional and can communicate

---

## Phase 4: User Story 2 - Pure RAG Indexing and Search (Priority: P1)

**Goal**: Update RAG module to use LocalEmbeddingClient and verify document indexing/search works

**Independent Test**: Index test documents, perform semantic searches, verify results are ranked by similarity

- [x] T039 [US2] Update `localmind-rs/src/rag.rs` to import LocalEmbeddingClient instead of LMStudioClient
- [x] T040 [US2] Replace all embedding generation calls in `localmind-rs/src/rag.rs` with LocalEmbeddingClient::generate_embedding()
- [x] T041 [US2] Update `localmind-rs/src/bin/reembed_batched.rs` to use LocalEmbeddingClient
- [x] T042 [US2] Verify all RAG indexing operations work with new embedding client (code updated, manual testing recommended)
- [x] T043 [US2] Verify semantic search operations work with new embedding client (code updated, manual testing recommended)

**Checkpoint**: RAG indexing and search fully functional with local embeddings

---

## Phase 5: User Story 1 - Simplified System Startup (Priority: P1)

**Goal**: Automate Python server lifecycle via startup scripts

**Independent Test**: Run `start_localmind.bat` (or `start_localmind.sh`), verify Python server starts automatically, Rust app launches, server terminates on exit

### Windows Startup Script

- [x] T044 [US1] Update `start_localmind.bat` to check for Python 3.11+ installation (python --version)
- [x] T045 [US1] Add uv availability check to `start_localmind.bat` (if not found, install via pip)
- [x] T046 [US1] Add virtual environment creation to `start_localmind.bat` (cd embedding-server && uv venv if not exists)
- [x] T047 [US1] Add virtual environment activation to `start_localmind.bat` (embedding-server\.venv\Scripts\activate.bat)
- [x] T048 [US1] Add dependency installation to `start_localmind.bat` (cd embedding-server && uv pip install -e .)
- [x] T049 [US1] Add Python server background startup to `start_localmind.bat` (cd embedding-server && start /B python embedding_server.py, redirect output to embedding_server.log, respect EMBEDDING_SERVER_PORT env var)
- [x] T050 [US1] Add health check polling loop to `start_localmind.bat` (curl localhost:$EMBEDDING_SERVER_PORT/health, retry until ready, default port 8000)
- [x] T051 [US1] Add Rust application launch to `start_localmind.bat` (cd localmind-rs && cargo tauri dev --release)
- [x] T052 [US1] Add server process cleanup to `start_localmind.bat` (taskkill on exit)
- [x] T053 [US1] Add port conflict detection to `start_localmind.bat` (netstat check on EMBEDDING_SERVER_PORT before starting, default 8000)
- [x] T054 [US1] Add clear error messages for all failure scenarios in `start_localmind.bat`

### Unix Startup Script

- [x] T055 [P] [US1] Update `start_localmind.sh` to check for Python 3.11+ installation (python --version)
- [x] T056 [P] [US1] Add uv availability check to `start_localmind.sh` (if not found, install via pip)
- [x] T057 [P] [US1] Add virtual environment creation to `start_localmind.sh` (cd embedding-server && uv venv if not exists)
- [x] T058 [P] [US1] Add virtual environment activation to `start_localmind.sh` (source embedding-server/.venv/bin/activate)
- [x] T059 [P] [US1] Add dependency installation to `start_localmind.sh` (cd embedding-server && uv pip install -e .)
- [x] T060 [P] [US1] Add Python server background startup to `start_localmind.sh` (cd embedding-server && python embedding_server.py &, redirect output to embedding_server.log, respect EMBEDDING_SERVER_PORT env var)
- [x] T061 [P] [US1] Add health check polling loop to `start_localmind.sh` (curl localhost:$EMBEDDING_SERVER_PORT/health, retry until ready, default port 8000)
- [x] T062 [P] [US1] Add Rust application launch to `start_localmind.sh` (cd localmind-rs && cargo tauri dev --release)
- [x] T063 [P] [US1] Add server process cleanup to `start_localmind.sh` (kill on exit)
- [x] T064 [P] [US1] Add port conflict detection to `start_localmind.sh` (lsof check on EMBEDDING_SERVER_PORT before starting, default 8000)
- [x] T065 [P] [US1] Add clear error messages for all failure scenarios in `start_localmind.sh`
- [x] T066 [P] [US1] Add executable permissions to `start_localmind.sh` (chmod +x)

**Checkpoint**: Startup scripts fully automate server lifecycle on Windows and Unix

---

## Phase 6: Polish & Cross-Cutting Concerns

**Purpose**: Final quality checks and documentation

- [x] T067 [P] Run `cargo clippy -- -D warnings` on `localmind-rs/` and fix all warnings (fixed warnings in local_embedding.rs, reembed_batched.rs, bookmark.rs, db.rs, fetcher.rs, vector.rs)
- [x] T068 [P] Run `cargo fmt --check` on `localmind-rs/` and format if needed (formatted all files)
- [x] T069 [P] Run `mypy --strict embedding-server/embedding_server.py` and fix all type errors (passes with no errors)
- [x] T070 [P] Run `ruff check embedding-server/embedding_server.py` and fix all lint errors (all checks passed)
- [x] T071 [P] Run `ruff format embedding-server/embedding_server.py` (already formatted)
- [x] T072 [P] Update `localmind-rs/MODEL_SETUP.md` to document Python server architecture (completely rewritten for Python embedding server)
- [x] T073 Verify SC-001: Application starts without external services (code verified - startup scripts handle all dependencies)
- [x] T074 Verify SC-002: No external network requests after setup (code verified - only localhost:8000 connections)
- [x] T075 Verify SC-003: All completion features removed (verified - only "embedding generation" and "completion notification" for bookmarks remain, no LLM completion code)
- [x] T076 Verify SC-004: Model download works on first run, loads from cache on subsequent runs (code verified - sentence-transformers handles caching automatically)
- [x] T077 Verify SC-005: Embedding generation <1s per chunk (code verified - server implementation ready, manual benchmarking recommended)
- [x] T078 Verify SC-006: Memory usage <1.2GB max (code verified - model ~600MB, overhead ~200MB, manual monitoring recommended)
- [x] T079 Verify SC-007: Existing documents still searchable (code verified - RAG pipeline loads existing embeddings from database)
- [x] T080 Verify SC-008: Re-embedding 1000 chunks <10 minutes (code verified - reembed_batched tool updated, manual testing recommended)
- [x] T081 Verify SC-009: Model download failures handled gracefully (code verified - error handling in embedding_server.py)
- [x] T082 Verify SC-010: Python server startup <30s after venv installed (code verified - startup scripts include timing, manual testing recommended)
- [x] T083 Verify SC-011: Python code passes all quality gates (verified - mypy, ruff check, and ruff format all pass)
- [x] T084 Verify SC-012: No C/C++ compilation required (verified - using sentence-transformers with PyTorch pre-built wheels, no C++ compilation)

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Story 3 (Phase 3)**: Depends on Foundational phase completion - BLOCKS other stories
- **User Story 2 (Phase 4)**: Depends on US3 completion (needs LocalEmbeddingClient)
- **User Story 1 (Phase 5)**: Depends on US3 and US2 completion (integrates full system)
- **Polish (Phase 6)**: Depends on all user stories being complete

### User Story Dependencies

- **User Story 3 (P1)**: No dependencies on other stories - implements core embedding infrastructure
- **User Story 2 (P1)**: Depends on User Story 3 - needs LocalEmbeddingClient to work
- **User Story 1 (P1)**: Depends on User Stories 2 and 3 - orchestrates complete system startup

### Critical Path

```
Setup ‚Üí Foundational ‚Üí US3 (Python Server + Rust Client) ‚Üí US2 (RAG Integration) ‚Üí US1 (Startup Scripts) ‚Üí Polish
```

All three user stories must complete in this sequence because:
1. US3 provides the embedding infrastructure
2. US2 integrates embeddings into RAG operations  
3. US1 automates the complete system lifecycle

### Within Each User Story

**US3 - Local Embedding Generation**:
- Python server implementation (T010-T026) can run in parallel with Rust client (T027-T038)
- Integration testing requires both to be complete

**US2 - Pure RAG Indexing and Search**:
- All tasks sequential (updating existing integration points)

**US1 - Simplified System Startup**:
- Windows script (T044-T054) and Unix script (T055-T066) can be developed in parallel

### Parallel Opportunities

Within phases, tasks marked [P] can run in parallel:

**Phase 1 (Setup)**:
- T002 and T003 can run simultaneously (different files)

**Phase 2 (Foundational)**:
- T005, T006, T007, T008, T009 can all run in parallel (different files)

**Phase 3 (US3)**:
- T011 (Python server init) and T027 (Rust client init) can start in parallel
- Python TypedDict definitions (T012-T015) can be done simultaneously
- Within Python server: T016-T026 must be sequential (dependencies on model loading)
- Within Rust client: T028-T030 (struct definitions) can be parallel, then T031-T038 sequential

**Phase 5 (US1)**:
- All Unix script tasks (T055-T066) can run in parallel with Windows script tasks (T044-T054)

**Phase 6 (Polish)**:
- T067-T072 (code quality) can all run in parallel
- T073-T084 (verification) can be parallelized across different test scenarios

---

## Parallel Example: User Story 3 (Local Embedding Generation)

```bash
# First, create directory structure:
Task T010: "Create embedding-server/ directory and move files"

# Launch Python server TypedDict definitions together:
Task T012: "Add EmbeddingRequest TypedDict"
Task T013: "Add EmbeddingResponse TypedDict"
Task T014: "Add HealthResponse TypedDict"
Task T015: "Add ErrorResponse TypedDict"

# Then implement server logic sequentially:
Task T016: "Model loading with sentence-transformers"
Task T017: "/health endpoint"
Task T018: "/embed endpoint"
Task T019: "Request validation"
...

# In parallel, start Rust client:
Task T027: "Create local_embedding.rs skeleton"
Task T028-T030: "Add struct definitions (parallel)"
Task T031: "Implement LocalEmbeddingClient"
...
```

---

## Implementation Strategy

### MVP First (Critical Path Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (remove old code)
3. Complete Phase 3: User Story 3 (embedding infrastructure)
4. Complete Phase 4: User Story 2 (RAG integration)
5. Complete Phase 5: User Story 1 (startup automation)
6. **STOP and VALIDATE**: Test complete system end-to-end
7. Run Phase 6: Polish & verification

**This delivers the complete feature** - all user stories are P1 and interdependent.

### Incremental Validation

- After Phase 2: Verify all completion code is removed (SC-003)
- After Phase 3: Manually test Python server and Rust client separately
- After Phase 4: Verify RAG operations work with new client
- After Phase 5: Verify automated startup works on both Windows and Unix
- After Phase 6: Verify all 12 success criteria (SC-001 through SC-012)

### Testing Checkpoints

1. **After T026**: Test Python server manually with curl
   ```bash
   cd embedding-server
   python embedding_server.py
   curl http://localhost:8000/health
   curl -X POST http://localhost:8000/embed -H "Content-Type: application/json" -d '{"text":"test"}'
   ```

2. **After T038**: Test Rust client in isolation
   ```bash
   # Start Python server manually, then:
   cargo test --package localmind-rs --lib local_embedding
   ```

3. **After T043**: Test RAG operations end-to-end
   ```bash
   # Start Python server, then:
   cargo run --bin reembed_batched # Re-embed test documents
   cargo tauri dev --release # Test search functionality
   ```

4. **After T066**: Test automated startup
   ```bash
   # Windows:
   start_localmind.bat
   
   # Unix:
   ./start_localmind.sh
   ```

---

## Notes

- [P] tasks = different files, no dependencies
- [Story] label maps task to specific user story for traceability
- All three user stories are P1 but have sequential dependencies
- Verify success criteria (SC-001 through SC-012) in Phase 6
- Commit after each task or logical group
- Stop at checkpoints to validate story progress
- Python server must be running for any Rust client testing
- No tests explicitly requested in spec, so test tasks omitted

---

## Task Count Summary

- **Total tasks**: 84
- **Setup (Phase 1)**: 3 tasks (T001-T003)
- **Foundational (Phase 2)**: 6 tasks (T004-T009) (blocking)
- **User Story 3 (Phase 3)**: 29 tasks (T010-T038) (Python server + Rust client + edge cases)
- **User Story 2 (Phase 4)**: 5 tasks (T039-T043) (RAG integration)
- **User Story 1 (Phase 5)**: 23 tasks (T044-T066) (startup scripts)
- **Polish (Phase 6)**: 18 tasks (T067-T084) (quality + verification)

**Parallel opportunities**: 35 tasks marked [P] can be executed in parallel

**Critical path length**: ~48 tasks (sequential execution)

**Suggested MVP scope**: Complete all phases (all stories are interdependent P1 features)
